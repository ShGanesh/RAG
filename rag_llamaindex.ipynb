{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG based Model for help in HR Policies\n",
    "\n",
    "The Language used in a few clauses was ambiguous, hence I wanted to parse all related policy documents for clauses related to my doubt. So, I created a Model that uses RAG and retrieves top K concerened clauses (which have >= a defined similarity score).\n",
    "\n",
    "Each step has been explained (Yes I redacted query information).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!pip install pypdf      \n",
    "!pip install -q transformers      \n",
    "!pip install langchain      \n",
    "!pip install sentence_transformers      \n",
    "!pip install llama_index      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import torch\n",
    "\n",
    "from llama_index import VectorStoreIndex, SimpleDirectoryReader, ServiceContext\n",
    "from llama_index.llms import HuggingFaceLLM\n",
    "from llama_index.prompts.prompts import SimpleInputPrompt\n",
    "from llama_index.core.retrievers import VectorIndexRetriever\n",
    "#from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "from llama_index.core.postprocessor import SimilarityPostprocessor\n",
    "\n",
    "# Embeddings\n",
    "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "from llama_index import ServiceContext\n",
    "from llama_index.embeddings import LangchainEmbedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Stuff\n",
    "SimpleDirectoryReader reads all `.pdf` files in the input directory recursively. This is loaded in variable 'documents'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = SimpleDirectoryReader(\n",
    "    input_dir = \"./contents/\",\n",
    "    recursive = True,\n",
    "    required_exts = [\".pdf\"],\n",
    ")\n",
    "\n",
    "documents = loader.load_data()\n",
    "print(f\"Loaded {len(documents)} docs.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we define our prompt. This is a wrapper around our front-end prompt.    \n",
    "> Note that we have done some role-play i.e we provide context to the model regarding its behaviour."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt=\"\"\"\n",
    "You are an HR Manager. Your goal is to answer questions as\n",
    "accurately as possible based on the instructions and context provided.\n",
    "If you do not know the answer, Say 'I do not know'.\n",
    "\"\"\"\n",
    "\n",
    "query_wrapper_prompt = SimpleInputPrompt(\"<|USER|>{query_str}<|MANAGER|>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLM using HuggingFace CLI\n",
    "\n",
    "Here we call a Huggingface LLm with standard parameters. We will be using the 7b parameter version of the [Llama2 model](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = HuggingFaceLLM(\n",
    "    context_window = 4096,\n",
    "    max_new_tokens = 256,\n",
    "    generate_kwargs = {\"temperature\": 0.0, \"do_sample\": False},\n",
    "    system_prompt = system_prompt,\n",
    "    query_wrapper_prompt = query_wrapper_prompt,\n",
    "    tokenizer_name = \"meta-llama/Llama-2-7b-chat-hf\",\n",
    "    model_name = \"meta-llama/Llama-2-7b-chat-hf\",\n",
    "    device_map = \"auto\",\n",
    "    model_kwargs = {\"torch_dtype\": torch.float16}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding\n",
    "Using BAAI (Beijing Academy of Artificial Intelligence) small BGE model for embeddings. Experiment with BERT.\n",
    "\n",
    "* Index: Creates indices of documents\n",
    "* Retriever: Finds similarity between query and documents, and outputs top k.\n",
    "* Response Synthesizer: \n",
    "* SimilarityPostProcessor: A PostPocessor that calculates similarity between query and documents. It is a filter with cutoff of 0.8 (arbitary)\n",
    "* QueryEngine: Pipelines retriever, synthesizer and similarity \n",
    "\n",
    "\n",
    "Question: Why different models for vectors and others? Why not use mpnet for both?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en-v1.5\")\n",
    "\n",
    "embed_model = LangchainEmbedding(\n",
    "    HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")\n",
    ")\n",
    "\n",
    "index = VectorStoreIndex.from_documents(\n",
    "    documents,\n",
    "    embed_model=embedding_model,\n",
    ")\n",
    "\n",
    "retriever = VectorIndexRetriever(\n",
    "    index = index,\n",
    "    similarity_top_k = 3,\n",
    ")\n",
    "\n",
    "response_synthesizer = get_response_synthesizer()\n",
    "\n",
    "query_engine = RetrieverQueryEngine(\n",
    "    retriever=retriever,\n",
    "    response_synthesizer=response_synthesizer,\n",
    "    node_postprocessors=[SimilarityPostprocessor(similarity_cutoff=0.8)],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "service_context = ServiceContext.from_defaults(\n",
    "    chunk_size = 1024,\n",
    "    llm = llm,\n",
    "    embed_model = embed_model\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = VectorStoreIndex.from_documents(documents, service_context = service_context)\n",
    "query_engine = index.as_query_engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resp = query_engine.query(\"Rules regarding <REDACTED>\")\n",
    "resp # Also provides data regarding page number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resp = query_engine.query(\"Regulations concerning IP\")\n",
    "resp "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
